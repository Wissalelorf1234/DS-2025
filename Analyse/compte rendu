# COMPTE RENDU : D√âTECTION DE FRAUDE PAR CARTE BANCAIRE

## TABLE DES MATI√àRES
1. Introduction
2. Description du Dataset
3. Contexte et Enjeux de la Fraude
4. Analyse Exploratoire des Donn√©es
5. Visualisations Graphiques
6. Analyse de D√©s√©quilibre des Classes
7. Mod√®les de Classification
8. Conclusion

---

## 1. INTRODUCTION

Le **Credit Card Fraud Detection Dataset** est un jeu de donn√©es crucial pour la s√©curit√© financi√®re, permettant de d√©velopper des syst√®mes de d√©tection automatique des transactions frauduleuses par carte bancaire.

**Probl√©matique** : Comment identifier avec pr√©cision les transactions frauduleuses parmi des millions de transactions l√©gitimes, tout en minimisant les faux positifs qui perturbent l'exp√©rience client ?

**URL** : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud

---

## 2. DESCRIPTION DU DATASET

### Caract√©ristiques g√©n√©rales
- **Nombre d'observations** : 284,807 transactions
- **P√©riode couverte** : Septembre 2013 (2 jours)
- **Nombre de variables** : 31
- **Variable cible** : Class (0 = l√©gitime, 1 = frauduleuse)

### Types de variables

**Variables transform√©es (PCA)** : V1, V2, V3, ..., V28
- 28 composantes principales issues d'une transformation PCA
- Variables anonymis√©es pour prot√©ger la confidentialit√© des donn√©es
- Repr√©sentent des caract√©ristiques cach√©es des transactions

**Variables non transform√©es** :
- **Time** : Secondes √©coul√©es entre chaque transaction et la premi√®re du dataset
- **Amount** : Montant de la transaction (en euros)
- **Class** : Variable cible (0 = transaction normale, 1 = fraude)

### D√©s√©quilibre des classes
- **Transactions l√©gitimes** : 284,315 (99.827%)
- **Transactions frauduleuses** : 492 (0.173%)
- **Ratio** : environ 1 fraude pour 577 transactions l√©gitimes

---

## 3. CONTEXTE ET ENJEUX DE LA FRAUDE

### Origine des donn√©es
Les donn√©es proviennent de transactions r√©elles effectu√©es par des porteurs de cartes bancaires europ√©ens. Pour des raisons de confidentialit√©, les variables originales ont √©t√© transform√©es via PCA (Principal Component Analysis).

### Impact √©conomique
- **Pertes mondiales estim√©es** : Plus de 30 milliards de dollars par an
- **Co√ªt moyen d'une fraude** : Variable selon le montant et le type de transaction
- **Co√ªt des faux positifs** : Frustration client, perte de confiance, blocages injustifi√©s

### Types de fraudes courants
1. **Fraude par carte perdue/vol√©e**
2. **Fraude sans pr√©sence de carte** (e-commerce)
3. **Skimming** (copie de carte)
4. **Fraude par ing√©nierie sociale**
5. **Fraude d'identit√©**

### D√©fis techniques
- ‚ö†Ô∏è **D√©s√©quilibre extr√™me** : Moins de 0.2% de fraudes
- ‚è±Ô∏è **Temps r√©el** : D√©cision en millisecondes
- üí∞ **Co√ªt asym√©trique** : Manquer une fraude co√ªte plus cher qu'un faux positif
- üîÑ **√âvolution constante** : Les fraudeurs adaptent leurs techniques

---

## 4. ANALYSE EXPLORATOIRE DES DONN√âES

### Statistiques descriptives de Amount

| Statistique | Valeur |
|-------------|--------|
| Moyenne | ‚Ç¨88.35 |
| M√©diane | ‚Ç¨22.00 |
| √âcart-type | ‚Ç¨250.12 |
| Minimum | ‚Ç¨0.00 |
| Maximum | ‚Ç¨25,691.16 |

### Distribution des montants

**Transactions l√©gitimes** :
- M√©diane : ‚Ç¨22.00
- Montant moyen : ‚Ç¨88.29

**Transactions frauduleuses** :
- M√©diane : ‚Ç¨9.25
- Montant moyen : ‚Ç¨122.21

**Observation cl√©** : Les transactions frauduleuses ont tendance √† avoir des montants plus √©lev√©s en moyenne, mais une m√©diane plus faible, sugg√©rant deux patterns distincts.

### Analyse temporelle
- **Dur√©e totale** : 172,792 secondes (~48 heures)
- **Distribution** : Les fraudes ne montrent pas de pattern temporel √©vident
- **Pics d'activit√©** : Variations normales du volume de transactions

---

## 5. VISUALISATIONS GRAPHIQUES

### Code Python pour les graphiques

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Chargement des donn√©es
df = pd.read_csv('creditcard.csv')

# Configuration style
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (14, 8)

# ========================================
# 1. DISTRIBUTION DES CLASSES
# ========================================

plt.figure(figsize=(10, 6))
class_counts = df['Class'].value_counts()
colors = ['#2ecc71', '#e74c3c']
plt.bar(['L√©gitime', 'Fraude'], class_counts.values, color=colors, edgecolor='black', linewidth=1.5)
plt.title('Distribution des Classes', fontsize=16, fontweight='bold')
plt.ylabel('Nombre de Transactions', fontsize=12)
plt.yscale('log')  # √âchelle logarithmique pour mieux voir la diff√©rence
for i, v in enumerate(class_counts.values):
    plt.text(i, v, f'{v:,}\n({v/len(df)*100:.3f}%)', ha='center', va='bottom', fontweight='bold')
plt.tight_layout()
plt.show()

# ========================================
# 2. DISTRIBUTION DES MONTANTS
# ========================================

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Toutes les transactions
axes[0].hist(df['Amount'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)
axes[0].set_xlabel('Montant (‚Ç¨)', fontsize=12)
axes[0].set_ylabel('Fr√©quence', fontsize=12)
axes[0].set_title('Distribution des Montants - Toutes Transactions', fontsize=14, fontweight='bold')
axes[0].set_xlim([0, 1000])  # Limite pour meilleure visualisation

# Comparaison L√©gitime vs Fraude
legitimate = df[df['Class'] == 0]['Amount']
fraud = df[df['Class'] == 1]['Amount']

axes[1].hist(legitimate, bins=50, alpha=0.6, label='L√©gitime', color='green', edgecolor='black')
axes[1].hist(fraud, bins=50, alpha=0.6, label='Fraude', color='red', edgecolor='black')
axes[1].set_xlabel('Montant (‚Ç¨)', fontsize=12)
axes[1].set_ylabel('Fr√©quence', fontsize=12)
axes[1].set_title('Comparaison des Montants : L√©gitime vs Fraude', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=12)
axes[1].set_xlim([0, 500])

plt.tight_layout()
plt.show()

# ========================================
# 3. BOXPLOT DES MONTANTS PAR CLASSE
# ========================================

plt.figure(figsize=(10, 6))
sns.boxplot(x='Class', y='Amount', data=df, palette=['#2ecc71', '#e74c3c'])
plt.xticks([0, 1], ['L√©gitime (0)', 'Fraude (1)'])
plt.xlabel('Type de Transaction', fontsize=12)
plt.ylabel('Montant (‚Ç¨)', fontsize=12)
plt.title('Distribution des Montants par Classe', fontsize=14, fontweight='bold')
plt.ylim([0, 300])  # Limite pour meilleure lisibilit√©
plt.tight_layout()
plt.show()

# ========================================
# 4. DISTRIBUTION TEMPORELLE
# ========================================

fig, axes = plt.subplots(2, 1, figsize=(14, 10))

# Toutes les transactions
axes[0].plot(df['Time'], df['Amount'], 'o', alpha=0.3, markersize=1, color='steelblue')
axes[0].set_xlabel('Temps (secondes)', fontsize=12)
axes[0].set_ylabel('Montant (‚Ç¨)', fontsize=12)
axes[0].set_title('Distribution Temporelle des Transactions', fontsize=14, fontweight='bold')
axes[0].set_ylim([0, 2000])

# Focus sur les fraudes
fraud_df = df[df['Class'] == 1]
axes[1].scatter(fraud_df['Time'], fraud_df['Amount'], color='red', alpha=0.7, s=50, edgecolor='black')
axes[1].set_xlabel('Temps (secondes)', fontsize=12)
axes[1].set_ylabel('Montant (‚Ç¨)', fontsize=12)
axes[1].set_title('Distribution Temporelle des Fraudes', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ========================================
# 5. DISTRIBUTION DES VARIABLES V1-V28
# ========================================

# S√©lection de quelques variables PCA importantes
important_features = ['V1', 'V2', 'V3', 'V4', 'V9', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17', 'V18']

fig, axes = plt.subplots(3, 4, figsize=(18, 12))
axes = axes.ravel()

for idx, feature in enumerate(important_features):
    axes[idx].hist(df[df['Class'] == 0][feature], bins=50, alpha=0.6, label='L√©gitime', color='green', density=True)
    axes[idx].hist(df[df['Class'] == 1][feature], bins=50, alpha=0.6, label='Fraude', color='red', density=True)
    axes[idx].set_title(f'Distribution de {feature}', fontweight='bold')
    axes[idx].legend()
    axes[idx].set_ylabel('Densit√©')

plt.suptitle('Distribution des Composantes PCA', fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.show()

# ========================================
# 6. STATISTIQUES DESCRIPTIVES
# ========================================

print("=" * 80)
print("STATISTIQUES DESCRIPTIVES")
print("=" * 80)

print("\nüìä INFORMATIONS G√âN√âRALES :")
print(f"Nombre total de transactions : {len(df):,}")
print(f"Transactions l√©gitimes : {len(df[df['Class']==0]):,} ({len(df[df['Class']==0])/len(df)*100:.3f}%)")
print(f"Transactions frauduleuses : {len(df[df['Class']==1]):,} ({len(df[df['Class']==1])/len(df)*100:.3f}%)")

print("\nüí∞ MONTANTS DES TRANSACTIONS :")
print(f"\nToutes transactions :")
print(f"  - Moyenne : ‚Ç¨{df['Amount'].mean():.2f}")
print(f"  - M√©diane : ‚Ç¨{df['Amount'].median():.2f}")
print(f"  - √âcart-type : ‚Ç¨{df['Amount'].std():.2f}")
print(f"  - Min : ‚Ç¨{df['Amount'].min():.2f}")
print(f"  - Max : ‚Ç¨{df['Amount'].max():.2f}")

legitimate_amount = df[df['Class'] == 0]['Amount']
fraud_amount = df[df['Class'] == 1]['Amount']

print(f"\nTransactions l√©gitimes :")
print(f"  - Moyenne : ‚Ç¨{legitimate_amount.mean():.2f}")
print(f"  - M√©diane : ‚Ç¨{legitimate_amount.median():.2f}")

print(f"\nTransactions frauduleuses :")
print(f"  - Moyenne : ‚Ç¨{fraud_amount.mean():.2f}")
print(f"  - M√©diane : ‚Ç¨{fraud_amount.median():.2f}")

print("\n‚è±Ô∏è INFORMATIONS TEMPORELLES :")
print(f"Dur√©e totale : {df['Time'].max():,} secondes (~{df['Time'].max()/3600:.1f} heures)")
```

---

## 6. ANALYSE DE D√âS√âQUILIBRE DES CLASSES

### Impact du d√©s√©quilibre

Le d√©s√©quilibre extr√™me (99.827% vs 0.173%) pose plusieurs d√©fis :

1. **Biais du mod√®le** : Tendance √† pr√©dire toujours "l√©gitime"
2. **M√©triques trompeuses** : 99.83% d'accuracy en pr√©disant toujours 0
3. **Apprentissage insuffisant** : Peu d'exemples de fraudes pour l'entra√Ænement

### Techniques de gestion du d√©s√©quilibre

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter

# Chargement des donn√©es
df = pd.read_csv('creditcard.csv')

X = df.drop('Class', axis=1)
y = df['Class']

# Split des donn√©es
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("=" * 80)
print("TECHNIQUES DE GESTION DU D√âS√âQUILIBRE")
print("=" * 80)

print(f"\nüìä Distribution originale :")
print(f"Classe 0 (L√©gitime) : {sum(y_train == 0):,}")
print(f"Classe 1 (Fraude) : {sum(y_train == 1):,}")
print(f"Ratio : {sum(y_train == 0) / sum(y_train == 1):.1f}:1")

# ========================================
# TECHNIQUE 1 : SMOTE (Over-sampling)
# ========================================

print("\n" + "=" * 80)
print("1Ô∏è‚É£ SMOTE (Synthetic Minority Over-sampling)")
print("=" * 80)

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print(f"Apr√®s SMOTE :")
print(f"Classe 0 : {sum(y_train_smote == 0):,}")
print(f"Classe 1 : {sum(y_train_smote == 1):,}")
print(f"Ratio : 1:1 (√©quilibr√©)")

# ========================================
# TECHNIQUE 2 : Under-sampling
# ========================================

print("\n" + "=" * 80)
print("2Ô∏è‚É£ Random Under-sampling")
print("=" * 80)

rus = RandomUnderSampler(random_state=42)
X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)

print(f"Apr√®s Under-sampling :")
print(f"Classe 0 : {sum(y_train_rus == 0):,}")
print(f"Classe 1 : {sum(y_train_rus == 1):,}")
print(f"Ratio : 1:1 (√©quilibr√©)")

# ========================================
# VISUALISATION
# ========================================

import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Original
axes[0].bar(['L√©gitime', 'Fraude'], [sum(y_train == 0), sum(y_train == 1)], 
            color=['green', 'red'], edgecolor='black', linewidth=1.5)
axes[0].set_title('Distribution Originale', fontsize=14, fontweight='bold')
axes[0].set_ylabel('Nombre de samples')
axes[0].set_yscale('log')

# SMOTE
axes[1].bar(['L√©gitime', 'Fraude'], [sum(y_train_smote == 0), sum(y_train_smote == 1)], 
            color=['green', 'red'], edgecolor='black', linewidth=1.5)
axes[1].set_title('Apr√®s SMOTE', fontsize=14, fontweight='bold')
axes[1].set_ylabel('Nombre de samples')

# Under-sampling
axes[2].bar(['L√©gitime', 'Fraude'], [sum(y_train_rus == 0), sum(y_train_rus == 1)], 
            color=['green', 'red'], edgecolor='black', linewidth=1.5)
axes[2].set_title('Apr√®s Under-sampling', fontsize=14, fontweight='bold')
axes[2].set_ylabel('Nombre de samples')

plt.tight_layout()
plt.show()
```

---

## 7. MOD√àLES DE CLASSIFICATION

### Code Python pour les mod√®les

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, precision_recall_curve
)
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# ========================================
# 1. PR√âPARATION DES DONN√âES
# ========================================

df = pd.read_csv('creditcard.csv')

X = df.drop('Class', axis=1)
y = df['Class']

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Normalisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Application de SMOTE
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

print("=" * 80)
print("PR√âPARATION DES DONN√âES")
print("=" * 80)
print(f"‚úì Taille du dataset : {len(df):,} transactions")
print(f"‚úì Train set : {len(X_train):,} samples")
print(f"‚úì Test set : {len(X_test):,} samples")
print(f"‚úì Train set apr√®s SMOTE : {len(X_train_balanced):,} samples")
print(f"‚úì Distribution apr√®s SMOTE : {Counter(y_train_balanced)}")

# ========================================
# 2. D√âFINITION DES MOD√àLES
# ========================================

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),
    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, eval_metric='logloss')
}

# ========================================
# 3. ENTRA√éNEMENT ET √âVALUATION
# ========================================

results = []
predictions = {}
probabilities = {}

print("\n" + "=" * 80)
print("ENTRA√éNEMENT ET √âVALUATION DES MOD√àLES")
print("=" * 80)

for name, model in models.items():
    print(f"\nüîÑ Entra√Ænement : {name}...")
    
    # Entra√Ænement
    model.fit(X_train_balanced, y_train_balanced)
    
    # Pr√©dictions
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None
    
    predictions[name] = y_pred
    probabilities[name] = y_pred_proba
    
    # Calcul des m√©triques
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else 0
    
    results.append({
        'Mod√®le': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'ROC-AUC': roc_auc
    })
    
    print(f"  ‚úì Accuracy : {accuracy:.4f}")
    print(f"  ‚úì Precision : {precision:.4f}")
    print(f"  ‚úì Recall : {recall:.4f}")
    print(f"  ‚úì F1-Score : {f1:.4f}")
    print(f"  ‚úì ROC-AUC : {roc_auc:.4f}")

# ========================================
# 4. TABLEAU R√âCAPITULATIF
# ========================================

results_df = pd.DataFrame(results).sort_values('F1-Score', ascending=False)

print("\n" + "=" * 80)
print("TABLEAU R√âCAPITULATIF DES PERFORMANCES")
print("=" * 80)
print(results_df.to_string(index=False))

# ========================================
# 5. VISUALISATIONS
# ========================================

# Graphique de comparaison des m√©triques
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']

for idx, metric in enumerate(metrics):
    ax = axes[idx // 2, idx % 2]
    sorted_df = results_df.sort_values(metric, ascending=True)
    ax.barh(sorted_df['Mod√®le'], sorted_df[metric], color=colors[idx], edgecolor='black', linewidth=1.5)
    ax.set_xlabel(metric, fontsize=12, fontweight='bold')
    ax.set_xlim([0, 1])
    ax.set_title(f'Comparaison : {metric}', fontsize=14, fontweight='bold')
    
    # Ajout des valeurs sur les barres
    for i, v in enumerate(sorted_df[metric]):
        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')

plt.tight_layout()
plt.show()

# ========================================
# 6. MATRICE DE CONFUSION (MEILLEUR MOD√àLE)
# ========================================

best_model_name = results_df.iloc[0]['Mod√®le']
best_predictions = predictions[best_model_name]

print(f"\nüèÜ Meilleur mod√®le : {best_model_name}")

cm = confusion_matrix(y_test, best_predictions)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
            xticklabels=['L√©gitime', 'Fraude'],
            yticklabels=['L√©gitime', 'Fraude'],
            annot_kws={'size': 16, 'weight': 'bold'})
plt.title(f'Matrice de Confusion - {best_model_name}', fontsize=14, fontweight='bold')
plt.ylabel('Valeur R√©elle', fontsize=12)
plt.xlabel('Valeur Pr√©dite', fontsize=12)
plt.tight_layout()
plt.show()

# Affichage d√©taill√© de la matrice
TN, FP, FN, TP = cm.ravel()
print(f"\nüìä D√©tails de la matrice de confusion :")
print(f"  ‚Ä¢ True Negatives (TN)  : {TN:,} (vraies l√©gitimes)")
print(f"  ‚Ä¢ False Positives (FP) : {FP:,} (fausses alertes)")
print(f"  ‚Ä¢ False Negatives (FN) : {FN:,} (fraudes manqu√©es) ‚ö†Ô∏è")
print(f"  ‚Ä¢ True Positives (TP)  : {TP:,} (fraudes d√©tect√©es) ‚úì")

# ========================================
# 7. COURBE ROC
# ========================================

plt.figure(figsize=(10, 8))

for name, proba in probabilities.items():
    if proba is not None:
        fpr, tpr, _ = roc_curve(y_test, proba)
        auc = roc_auc_score(y_test, proba)
        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)

plt.plot([0, 1], [0, 1], 'k--', label='Hasard (AUC = 0.500)', linewidth=2)
plt.xlabel('Taux de Faux Positifs', fontsize=12)
plt.ylabel('Taux de Vrais Positifs', fontsize=12)
plt.title('Courbes ROC - Comparaison des Mod√®les', fontsize=14, fontweight='bold')
plt.legend(loc='lower right', fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# ========================================
# 8. COURBE PRECISION-RECALL
# ========================================

plt.figure(figsize=(10, 8))

for name, proba in probabilities.items():
    if proba is not None:
        precision_vals, recall_vals, _ = precision_recall_curve(y_test, proba)
        plt.plot(recall_vals, precision_vals, label=name, linewidth=2)

plt.xlabel('Recall', fontsize=12)
plt.ylabel('Precision', fontsize=12)
plt.title('Courbes Precision-Recall', fontsize=14, fontweight='bold')
plt.legend(loc='lower left', fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# ========================================
# 9. RAPPORT DE CLASSIFICATION
# ========================================

print("\n" + "=" * 80)
print(f"RAPPORT DE CLASSIFICATION D√âTAILL√â - {best_model_name}")
print("=" * 80)
print(classification_report(y_test, best_predictions, target_names=['L√©gitime', 'Fraude']))

print("\n‚úì Analyse de classification termin√©e")
```

### R√©sultats attendus

Les mod√®les bas√©s sur les ensembles obtiennent g√©n√©ralement les meilleures performances :

| M√©trique | Random Forest | XGBoost | Gradient Boosting |
|----------|---------------|---------|-------------------|
| **Accuracy** | 0.9995 | 0.9996 | 0.9994 |
| **Precision** | 0.95 - 0.98 | 0.96 - 0.99 | 0.94 - 0.97 |
| **Recall** | 0.85 - 0.92 | 0.88 - 0.94 | 0.83 - 0.90 |
| **F1-Score** | 0.90 - 0.95 | 0.92 - 0.96 | 0.88 - 0.93 |
| **ROC-AUC** | 0.97 - 0.99 | 0.98 - 0.99 | 0.96 - 0.98 |

---

## 8. CONCLUSION

### Points cl√©s

‚úÖ **Dataset r√©aliste** : 284,807 transactions authentiques avec anonymisation PCA  
‚úÖ **D√©fi majeur** : D√©s√©quilibre extr√™me (0.173% de fraudes)  
‚úÖ **Techniques essentielles** : SMOTE, ajustement des seuils, m√©triques adapt√©es  
‚úÖ **Mod√®les performants** : XGBoost et Random Forest excellent avec ROC-AUC > 0.98  
‚úÖ **M√©trique prioritaire** : Le Recall est crucial (minimiser les fraudes manqu√©es)

### Recommandations

1. **Feature Engineering**
   - Cr√©er des agr√©gations temporelles (transactions par heure/jour)
   - Calculer des statistiques par utilisateur (fr√©quence, montant moyen)
   - Analyser les patterns de comportement

2. **Optimisation des seuils**
   - Ajuster le seuil de d√©cision selon le co√ªt m√©tier
   - Privil√©gier le Recall pour minimiser les fraudes manqu√©es
   - √âquilibrer avec les faux positifs pour l'exp√©rience client

3. **Validation robuste**
   - Validation crois√©e stratifi√©e
   - √âvaluation sur donn√©es temporellement s√©par√©es
   - Tests sur nouveaux types de fraudes

4. **D√©ploiement en production**
   - Scoring en temps r√©el (< 100ms)
   - Syst√®me de r√®gles m√©tier compl√©mentaire
   - Monitoring continu des performances
   - R√©entra√Ænement r√©gulier du mod√®le

5. **Gestion des co√ªts**
   - Co√ªt d'une fraude manqu√©e : √©lev√© (perte du montant + frais)
   - Co√ªt d'un faux positif : mod√©r√© (friction client)
   - Optimiser selon la matrice de co√ªts m√©tier

### Applications pratiques

- üè¶ **Syst√®mes bancaires** : D√©tection en temps r√©el des transactions suspectes
- üí≥ **√âmetteurs de cartes** : Scoring de risque pour chaque transaction
- üõ°Ô∏è **Pr√©vention** : Identification des patterns de fraude √©mergents
- üìä **Analyse forensique** : Investigation post-fraude
- ü§ñ **Automatisation** : R√©duction de la r√©vision manuelle

### Lim
